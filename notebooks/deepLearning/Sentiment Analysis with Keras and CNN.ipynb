{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for IMBD movie reviews using Keras and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from keras.utils import data_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and explore data\n",
    "\n",
    "Keras has the imdb dataset as part of its distribution but that dataset is already pre processed. I want to cover pre processing steps because that is an important part of Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84090880/84125825 [============================>.] - ETA: 0sUntaring file...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "imdb_path = data_utils.get_file('aclImdb', \n",
    "                                'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', untar=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: show histogram for review length and some graph with word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_reviews(path):\n",
    "    reviews = []\n",
    "    for fname in sorted(os.listdir(path)):  \n",
    "        fpath = os.path.join(path, fname)\n",
    "        f = open(fpath)\n",
    "        reviews.append(f.read())\n",
    "        f.close()\n",
    "    return reviews\n",
    "\n",
    "positive_reviews = load_reviews(os.path.join(imdb_path, 'train/pos')) \n",
    "negative_reviews = load_reviews(os.path.join(imdb_path, 'train/neg')) \n",
    "\n",
    "all_reviews = []\n",
    "all_reviews.extend(positive_reviews)\n",
    "all_reviews.extend(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation\n",
    "\n",
    "Original data is in text format. In order to be able to feed it into a neural network it needs to be converted into tensors first. \n",
    "\n",
    "The first step is tokenizing the reviews. The tokenizer converts each review into a sequence of integers with each integer representing the index of the word in a dictionary. Next the sequences are padded so all of them have the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL SENTENCE\n",
      "---------------\n",
      "This was a must see documentary for me when I missed the opportunity in 2004, so I was definitely going to watch the repeat. I really sympathised with the main character of the film, because, this is true, I have a milder condition of the skin problem he had, Dystrophic Epidermolysis Bullosa (EB). This is a sad, sometimes amusing and very emotional documentary about a boy with a terrible skin disorder. Jonny Kennedy speaks like a kid (because of wasting vocal muscle) and never went through puberty, but he is 36 years old. Most sympathising moments are seeing his terrible condition, and pealing off his bandages. Jonny had quite a naughty sense of humour, he even narrated from beyond the grave when showing his body in a coffin. He tells his story with the help of his mother, Edna Kennedy, his older brother and celebrity model, and Jonny's supporter, Nell McAndrew. It won the BAFTAs for Best Editing and Best New Director (Factual), and it was nominated for Best Sound (Factual) and the Flaherty Documentary Award. It was number 10 on The 100 Greatest TV Treats 2004. A must see documentary!\n",
      "TOKENIZED SEQUENCE\n",
      "---------------\n",
      "[11, 13, 3, 212, 64, 661, 15, 69, 51, 10, 1046, 1, 1431, 8, 3829, 35, 10, 13, 404, 167, 5, 103, 1, 3257, 10, 63, 16, 1, 290, 106, 4, 1, 19, 85, 11, 6, 280, 10, 25, 3, 3222, 4, 1, 2388, 436, 26, 66, 11, 6, 3, 616, 515, 1136, 2, 52, 918, 661, 41, 3, 427, 16, 3, 391, 2388, 4221, 2463, 37, 3, 551, 85, 4, 3118, 2, 112, 432, 140, 18, 26, 6, 150, 151, 88, 385, 23, 316, 24, 391, 3222, 2, 122, 24, 66, 176, 3, 278, 4, 1282, 26, 57, 36, 721, 1, 2619, 51, 797, 24, 645, 8, 3, 26, 713, 24, 62, 16, 1, 336, 4, 24, 449, 4221, 24, 919, 594, 2, 4414, 2183, 2, 9, 1196, 1, 15, 115, 799, 2, 115, 159, 164, 2, 9, 13, 2302, 15, 115, 478, 2, 1, 661, 1341, 9, 13, 609, 155, 20, 1, 1242, 830, 245, 4402, 3829, 3, 212, 64, 661]\n",
      "PADDED SEQUENCE\n",
      "---------------\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0   11   13    3  212   64  661   15   69\n",
      "   51   10 1046    1 1431    8 3829   35   10   13  404  167    5  103    1\n",
      " 3257   10   63   16    1  290  106    4    1   19   85   11    6  280   10\n",
      "   25    3 3222    4    1 2388  436   26   66   11    6    3  616  515 1136\n",
      "    2   52  918  661   41    3  427   16    3  391 2388 4221 2463   37    3\n",
      "  551   85    4 3118    2  112  432  140   18   26    6  150  151   88  385\n",
      "   23  316   24  391 3222    2  122   24   66  176    3  278    4 1282   26\n",
      "   57   36  721    1 2619   51  797   24  645    8    3   26  713   24   62\n",
      "   16    1  336    4   24  449 4221   24  919  594    2 4414 2183    2    9\n",
      " 1196    1   15  115  799    2  115  159  164    2    9   13 2302   15  115\n",
      "  478    2    1  661 1341    9   13  609  155   20    1 1242  830  245 4402\n",
      " 3829    3  212   64  661]\n"
     ]
    }
   ],
   "source": [
    "# Only used most frequently MAX_NB_WORDS used words. The words are indexed such that lower indexes correspond\n",
    "# to more frequently used words\n",
    "MAX_NB_WORDS = 5000\n",
    "SEQUENCE_LENGTH = 500\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_reviews)\n",
    "\n",
    "# Unknown words or words that are not frequently used are ingored.\n",
    "sequences = tokenizer.texts_to_sequences(all_reviews)\n",
    "\n",
    "# pad sequences with 0 so they have the same length. Longer sequences are reduced to SEQUENCE_LENGTH. \n",
    "reviews_vectors = pad_sequences(sequences, maxlen=SEQUENCE_LENGTH)\n",
    "labels = np.zeros(len(all_reviews), dtype=int)\n",
    "labels[0:len(positive_reviews)] = 1\n",
    "\n",
    "def sentence_info(index):\n",
    "    print('ORIGINAL SENTENCE\\n---------------')\n",
    "    print(all_reviews[index])\n",
    "    print('TOKENIZED SEQUENCE\\n---------------')\n",
    "    print(sequences[index])\n",
    "    print('PADDED SEQUENCE\\n---------------')\n",
    "    print(reviews_vectors[index])\n",
    "\n",
    "sentence_info(1000)\n",
    "\n",
    "#shuffle the data\n",
    "indices = np.arange(reviews_vectors.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "reviews_vectors = reviews_vectors[indices]\n",
    "labels = labels[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 50)       250000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 498, 250)      37750       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 1, 250)        0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 250)           0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 250)           62750       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 250)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 250)           0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             251         activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 1)             0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 350751\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model parameters\n",
    "embedding_dims = 50\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first layer is an embedding layer. This transforms each number in the input sequence into a vector with embeding_dims\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    "                    embedding_dims,\n",
    "                    input_length=SEQUENCE_LENGTH,\n",
    "                    dropout=0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(pool_length=model.output_shape[1]))\n",
    "\n",
    "# Output from the Convolution layer is flattened so it can be fed into a Dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Since this is a binary classification the output from the Dense layer is fed into another layer with a single\n",
    "# neuron. This in turn is activated by a sigmoid. A sigmoid makes sense in this case because it can be interpreted as\n",
    "# a probability.\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py:89: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 381s - loss: 0.4479 - acc: 0.7734 - val_loss: 0.2971 - val_acc: 0.8792\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 385s - loss: 0.2901 - acc: 0.8794 - val_loss: 0.2747 - val_acc: 0.8904\n"
     ]
    }
   ],
   "source": [
    "#training parameters\n",
    "batch_size = 32\n",
    "nb_epoch = 2\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history  = model.fit(reviews_vectors, labels,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Predict sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00883079]\n",
      " [ 0.71472728]]\n"
     ]
    }
   ],
   "source": [
    "def process_data(tokenizer, reviews):\n",
    "    sequences = tokenizer.texts_to_sequences(reviews)\n",
    "    return pad_sequences(sequences, maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "review1 = \"\"\"This movie is horrendous on so many levels - patronising to Christians, incredibly bad plot and direction, the acting and dialogue is as comfortable as a dentist's visit. I suspect the whole thing is purely a desperate attempt by Cameron to pander to under-educated Christians in an effort to earn their hard earned dollars. Either that, or it is just a giant ego trip for him. I had no preconceptions before I went to watch this, and left confused as to the amount of utter garbage wedged into what seemed like an eternity of boredom. Even worse, he is asking his Christian supporters to bear false witness by skewing the votes here and on rotten tomatoes. Hypocritical? I'll let you decide.\"\"\"\n",
    "review2 = \"\"\"I had the rather intense privilege to view James Cameron's much anticipated $400 million budget return to the directing scene, Avatar, at the Empire Leicester Square in London.\n",
    "Where to begin! The visuals in this pieces was groundbreaking. He did it with the Terminator series and then Titanic, so one would expect Cameron to deliver... and HE DID! The visual are by far some of the sharpest CGI I have seen. You could almost say that there is a disquiet that follows Cameron's soul, as there is no other possibility of this strong and intensified quality. Its production design and visual effects are both noteworthy and it will get its praise upon official release.\n",
    "What it was lacking that really should have shaped the movie is its character/story. I was expecting a complex and believable plot, but was left with a movie with mostly strong visuals. What most sci-fi lovers desire is mind-bending philosophies, fantasy and exploration and limitations of our or outer species. If it was not for this factor, I would give this a 9.5 vote.\n",
    "Avatar will be a success, not only because of Cameron's legacy, but by very intelligent and viral marketing. Avatar have had a powerful marketing technique that assembles other successful blockbusters, such as The Blair Witch project (you all remember it), The Dark Knight (Joker invades the world) and also, the current production The Artifice (the-artifice.com) that is intelligently targeting the market.\n",
    "Kudos to Cameron, Avatar is one of the (if not The) movie of the year. I could get in trouble for sharing this with you guys so early, so please click Yes on \"Was the above comment useful to you?\" as a thank you.\"\"\"\n",
    "data = process_data(tokenizer,[review1,review2])\n",
    "\n",
    "print(model.predict(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
